{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBEizLTZXdmC"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Skip restarting message in Colab\n",
        "import sys; modules = list(sys.modules.keys())\n",
        "for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "\n",
        "!pip install unsloth vllm\n",
        "!pip install --upgrade pillow\n",
        "!pip install rapidfuzz datasets\n",
        "# If you are running this notebook on local, you need to install `diffusers` too\n",
        "# !pip install diffusers\n",
        "# Temporarily install a specific TRL nightly version\n",
        "!pip install git+https://github.com/huggingface/trl.git@e95f9fb74a3c3647b86f251b7e230ec51c64b72b\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from rapidfuzz import fuzz\n",
        "from datasets import load_dataset, Dataset\n",
        "import re\n",
        "from unsloth import is_bfloat16_supported, FastLanguageModel, PatchFastRL"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PatchFastRL(\"GRPO\", FastLanguageModel)"
      ],
      "metadata": {
        "id": "_rl1q7hmXgdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "max_seq_length = 1000 # Can increase for longer reasoning traces\n",
        "lora_rank = 8 # Larger rank = smarter, but slower\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True, # False for LoRA 16bit\n",
        "    fast_inference = True, # Enable vLLM fast inference\n",
        "    max_lora_rank = lora_rank,\n",
        "    gpu_memory_utilization = 0.2, # Reduce if out of memory\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ], # Remove QKVO if out of memory\n",
        "    lora_alpha = lora_rank,\n",
        "    use_gradient_checkpointing = \"unsloth\", # Enable long context finetuning\n",
        "    random_state = 3407,\n",
        ")"
      ],
      "metadata": {
        "id": "AP6TLkfsXh6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "!nvidia-smi\n"
      ],
      "metadata": {
        "id": "ELpGixlTXiPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Load and Process the Jailbreak Dataset ---\n",
        "\n",
        "jailbreak_dataset = load_dataset('TrustAIRLab/in-the-wild-jailbreak-prompts', 'jailbreak_2023_05_07')['train']\n",
        "jailbreak_df = pd.DataFrame(jailbreak_dataset)\n",
        "\n",
        "print(\"\\nJailbreak Dataset Info:\")\n",
        "print(f\"Total rows: {len(jailbreak_df)}\")\n",
        "print(f\"Unique prompts: {jailbreak_df['prompt'].nunique()}\")\n",
        "print(\"\\nSample of jailbreak prompts:\")\n",
        "print(jailbreak_df[['prompt']].head(3))\n",
        "\n",
        "# Remove exact duplicates from jailbreak dataset\n",
        "jailbreak_df_unique = jailbreak_df.drop_duplicates(subset=['prompt'])\n",
        "print(f\"\\nShape after exact duplicates: {len(jailbreak_df_unique)} rows\")\n",
        "\n",
        "def find_similar_prompts(prompts, threshold=90):\n",
        "    \"\"\"\n",
        "    Find groups of similar prompts using fuzzy matching.\n",
        "    \"\"\"\n",
        "    similar_groups = {}\n",
        "    processed = set()\n",
        "    print(\"Finding similar prompts...\")\n",
        "    for i in tqdm(range(len(prompts))):\n",
        "        if i in processed:\n",
        "            continue\n",
        "        current_prompt = prompts[i]\n",
        "        group = [i]\n",
        "        for j in range(i + 1, len(prompts)):\n",
        "            if j in processed:\n",
        "                continue\n",
        "            if fuzz.ratio(current_prompt, prompts[j]) >= threshold:\n",
        "                group.append(j)\n",
        "                processed.add(j)\n",
        "        if len(group) > 1:\n",
        "            similar_groups[i] = group\n",
        "        processed.add(i)\n",
        "    return similar_groups\n",
        "\n",
        "\n",
        "# Remove fuzzy duplicates from jailbreak dataset\n",
        "jailbreak_prompts = jailbreak_df['prompt'].tolist()\n",
        "similar_groups = find_similar_prompts(jailbreak_prompts, threshold=90)\n",
        "indices_to_keep = set(range(len(jailbreak_prompts))) - {\n",
        "    idx for group in similar_groups.values() for idx in group[1:]\n",
        "}\n",
        "jailbreak_df_fuzzy_unique = jailbreak_df.iloc[list(indices_to_keep)]\n"
      ],
      "metadata": {
        "id": "z9nJ-WKbXicV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 2. Load Regular Prompts and Combine Datasets ---\n",
        "\n",
        "regular_dataset = load_dataset('TrustAIRLab/in-the-wild-jailbreak-prompts', 'regular_2023_05_07')['train']\n",
        "regular_df = pd.DataFrame(regular_dataset)\n",
        "\n",
        "# Sample an equal number of regular prompts\n",
        "num_jailbreak = len(jailbreak_df_fuzzy_unique)\n",
        "regular_df_sampled = regular_df.sample(n=num_jailbreak, random_state=42)\n",
        "\n",
        "# Add labels\n",
        "jailbreak_df_fuzzy_unique['label'] = \"jailbreak\"\n",
        "regular_df_sampled['label'] = \"safe\"\n",
        "\n",
        "# Combine and shuffle\n",
        "combined_df = pd.concat([jailbreak_df_fuzzy_unique, regular_df_sampled]).sample(frac=1, random_state=42)\n",
        "\n",
        "# Convert to HuggingFace Dataset format\n",
        "combined_dataset = Dataset.from_pandas(combined_df)\n"
      ],
      "metadata": {
        "id": "hMtVDE6hXilf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 3. Define Prompt Template and Formatting Function ---\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Respond in the following format:\n",
        "<reasoning>\n",
        "...\n",
        "</reasoning>\n",
        "<answer>\n",
        "...\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "# Set the maximum sequence length\n",
        "MAX_LENGTH = 1000  # Hard limit\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    \"\"\"\n",
        "    Formats the dataset examples into the desired prompt-response structure and tokenizes them.\n",
        "    \"\"\"\n",
        "    prompts = examples[\"prompt\"]\n",
        "    labels = examples[\"label\"]\n",
        "\n",
        "    # Map labels to response text\n",
        "    responses = [\"jailbreak\" if label == \"jailbreak\" else \"safe\" for label in labels]\n",
        "\n",
        "    texts = []\n",
        "    for prompt, response in zip(prompts, responses):\n",
        "        text = (\n",
        "            f\"{SYSTEM_PROMPT}\\n### Instruction:\\n{prompt}\\n### Response:\\n\"\n",
        "            f\"<reasoning>\\n...\\n</reasoning>\\n\"\n",
        "            f\"<answer>\\n{response}\\n</answer>\"\n",
        "        )\n",
        "        texts.append(text)\n",
        "\n",
        "    # Tokenize with truncation and padding\n",
        "    tokenized = tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\",\n",
        "        add_special_tokens=True\n",
        "    )\n",
        "\n",
        "    # Force the final token in each sequence to be the EOS token\n",
        "    input_ids = tokenized[\"input_ids\"]\n",
        "    input_ids[:, -1] = tokenizer.eos_token_id\n",
        "\n",
        "    # Decode the tokenized input_ids back into text so that the text field is also truncated.\n",
        "    truncated_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in input_ids]\n",
        "\n",
        "    return {\n",
        "        \"prompt\": truncated_texts,  # Overwrite original prompt field\n",
        "        \"labels\": responses,\n",
        "        \"input_ids\": input_ids.tolist(),\n",
        "        \"attention_mask\": tokenized[\"attention_mask\"].tolist(),\n",
        "    }\n"
      ],
      "metadata": {
        "id": "A9G0b-DKXiuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 4. Map the Formatting Function over the Dataset ---\n",
        "\n",
        "# Note: Ensure that this is the only mapping call for formatting.\n",
        "dataset = combined_dataset.map(formatting_prompts_func, batched=True)\n",
        "# After dataset mapping\n",
        "def verify_token_length(example):\n",
        "    tokens = tokenizer(example[\"prompt\"])[\"input_ids\"]\n",
        "    assert len(tokens) <= MAX_LENGTH, f\"Token length {len(tokens)} exceeds limit!\"\n",
        "    return example\n",
        "\n",
        "dataset = dataset.map(verify_token_length)\n"
      ],
      "metadata": {
        "id": "jeRnmIr4Xi2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 5. Prepare Model, Trainer, and Start Training ---\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "training_args = GRPOConfig(\n",
        "    use_vllm=True,\n",
        "    learning_rate=3e-6,  # Lower LR for more stable training\n",
        "    adam_beta1=0.9,\n",
        "    adam_beta2=0.999,  # Closer to default AdamW params\n",
        "    weight_decay=0.1,\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    logging_steps=50,  # Log more often\n",
        "    bf16=True,  # Disable mixed precision\n",
        "    fp16=False,\n",
        "    per_device_train_batch_size=4,  # Increase batch size if memory allows\n",
        "    gradient_accumulation_steps=2,  # Smoother training\n",
        "    num_generations=6,\n",
        "    max_prompt_length=MAX_LENGTH,\n",
        "    max_completion_length=200,\n",
        "    max_steps=100,  # Train longer for better generalization\n",
        "    save_steps=500,  # Save more frequently\n",
        "    max_grad_norm=0.5,  # Reduce aggressive gradient clipping\n",
        "    num_train_epochs = 2,\n",
        "    report_to = \"none\",\n",
        "    output_dir=\"outputs\"\n",
        ")\n",
        "prompt_to_label = {example[\"prompt\"]: example[\"label\"].lower() for example in dataset}\n",
        "\n",
        "def correctness_reward_func(prompts, completions, **kwargs):\n",
        "    \"\"\"\n",
        "    Assigns reward based on correctness with fallback for empty outputs.\n",
        "    \"\"\"\n",
        "    extracted_responses = []\n",
        "    for r in completions:\n",
        "        if \"<answer>\" in r:\n",
        "            extracted_responses.append(r.split(\"<answer>\")[-1].split(\"</answer>\")[0].strip())\n",
        "        else:\n",
        "            extracted_responses.append(\"\")\n",
        "\n",
        "    rewards = []\n",
        "    for r, a in zip(extracted_responses, prompts):\n",
        "        correct_label = prompt_to_label.get(a, \"\").lower()\n",
        "        if not r:  # If empty response, penalize\n",
        "            rewards.append(-1.0)\n",
        "        elif r.lower() == correct_label:\n",
        "            rewards.append(2.0)\n",
        "        elif fuzz.ratio(r.lower(), correct_label) >= 85:  # Partial match\n",
        "            rewards.append(1.5)\n",
        "        else:\n",
        "            rewards.append(0.0)\n",
        "    return rewards\n",
        "\n",
        "\n",
        "\n",
        "def format_reward_func(completions, **kwargs):\n",
        "    \"\"\"\n",
        "    Rewards completions that follow the structured XML format.\n",
        "    \"\"\"\n",
        "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
        "    responses = completions\n",
        "    return [0.5 if re.match(pattern, r) else 0.0 for r in responses]\n",
        "\n",
        "def reasoning_reward_func(completions, **kwargs):\n",
        "    \"\"\"\n",
        "    Rewards completions that include a reasoning step before answering.\n",
        "    \"\"\"\n",
        "    responses = completions\n",
        "    return [0.5 if \"<reasoning>\" in r and \"</reasoning>\" in r else 0.0 for r in responses]\n",
        "\n"
      ],
      "metadata": {
        "id": "coAL_OArXjAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(torch.bfloat16)\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    reward_funcs=[\n",
        "        correctness_reward_func,\n",
        "        format_reward_func,\n",
        "        reasoning_reward_func\n",
        "    ],\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "gwDPw6GjYtbB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}