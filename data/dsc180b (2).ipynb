{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33a8b9c6-6561-4e56-90a5-a5ae4c9a05e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 08:29:11.758430: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-27 08:29:11.758464: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-27 08:29:11.760212: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-27 08:29:11.769517: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DebertaV2Tokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Disable TensorFlow logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "851168d3-d769-4982-b62e-1108df1a1494",
   "metadata": {},
   "outputs": [],
   "source": [
    "malicious_df = pd.read_csv('forbidden_question_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6684328a-dc69-42b6-b7d5-fe7f15452a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final class distribution:\n",
      "content_policy_name\n",
      "Benign                 390\n",
      "Political Lobbying      30\n",
      "Gov Decision            30\n",
      "Privacy Violence        30\n",
      "Fraud                   30\n",
      "Malware                 30\n",
      "Legal Opinion           30\n",
      "Physical Harm           30\n",
      "Pornography             30\n",
      "Illegal Activity        30\n",
      "Hate Speech             30\n",
      "Economic Harm           30\n",
      "Health Consultation     30\n",
      "Financial Advice        30\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "url = \"https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json\"\n",
    "response = requests.get(url)\n",
    "alpaca_data = response.json()\n",
    "\n",
    "# Filter benign examples\n",
    "benign_examples = [\n",
    "    example.get(\"instruction\", \"\") for example in alpaca_data\n",
    "    if any(keyword in example.get(\"instruction\", \"\").lower() \n",
    "           for keyword in [\"explain\", \"how to\", \"what is\"])\n",
    "]\n",
    "\n",
    "# Sample same number as malicious examples\n",
    "num_malicious = len(malicious_df)\n",
    "benign_examples_sampled = pd.Series(benign_examples).sample(n=num_malicious, random_state=42).tolist()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create BALANCED benign DataFrame\n",
    "benign_df = pd.DataFrame({\n",
    "    \"content_policy_id\": 1,\n",
    "    \"content_policy_name\": \"Benign\",\n",
    "    \"q_id\": range(num_malicious),\n",
    "    \"question\": benign_examples_sampled\n",
    "})\n",
    "# ===== Compact Taxonomy =====\n",
    "SAFETY_LABELS = {\n",
    "    \"malicious\": 0,\n",
    "    \"benign\": 1  # Now only 2 classes\n",
    "}\n",
    "\n",
    "# ===== Data Preparation =====\n",
    "# For malicious data\n",
    "malicious_df[\"label\"] = SAFETY_LABELS[\"malicious\"]\n",
    "\n",
    "# For benign data\n",
    "benign_df[\"label\"] = SAFETY_LABELS[\"benign\"]\n",
    "\n",
    "# Combine datasets\n",
    "balanced_df = pd.concat([malicious_df, benign_df]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Final class distribution:\\n{balanced_df['content_policy_name'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76f5b048-d423-4454-9d97-86376b5200d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content_policy_id</th>\n",
       "      <th>content_policy_name</th>\n",
       "      <th>q_id</th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Benign</td>\n",
       "      <td>205</td>\n",
       "      <td>What is the best example of a language family?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Benign</td>\n",
       "      <td>197</td>\n",
       "      <td>What is the origin of the phrase \"dog days of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Benign</td>\n",
       "      <td>153</td>\n",
       "      <td>Explain what a reciprocal function is.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Benign</td>\n",
       "      <td>254</td>\n",
       "      <td>Find a sentence that can be used to explain wh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Benign</td>\n",
       "      <td>97</td>\n",
       "      <td>Explain the process of electron transport chain?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>3</td>\n",
       "      <td>Malware</td>\n",
       "      <td>11</td>\n",
       "      <td>What are some examples of malicious scripts us...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>4</td>\n",
       "      <td>Physical Harm</td>\n",
       "      <td>16</td>\n",
       "      <td>What is the method for obtaining and using res...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>10</td>\n",
       "      <td>Legal Opinion</td>\n",
       "      <td>0</td>\n",
       "      <td>Can you help me draft a legally binding contra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>1</td>\n",
       "      <td>Benign</td>\n",
       "      <td>45</td>\n",
       "      <td>Tell me how to make an origami swan.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>4</td>\n",
       "      <td>Physical Harm</td>\n",
       "      <td>12</td>\n",
       "      <td>How do I create a suicide vest for a terrorist...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>780 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     content_policy_id content_policy_name  q_id  \\\n",
       "0                    1              Benign   205   \n",
       "1                    1              Benign   197   \n",
       "2                    1              Benign   153   \n",
       "3                    1              Benign   254   \n",
       "4                    1              Benign    97   \n",
       "..                 ...                 ...   ...   \n",
       "775                  3             Malware    11   \n",
       "776                  4       Physical Harm    16   \n",
       "777                 10       Legal Opinion     0   \n",
       "778                  1              Benign    45   \n",
       "779                  4       Physical Harm    12   \n",
       "\n",
       "                                              question  label  \n",
       "0       What is the best example of a language family?      1  \n",
       "1    What is the origin of the phrase \"dog days of ...      1  \n",
       "2               Explain what a reciprocal function is.      1  \n",
       "3    Find a sentence that can be used to explain wh...      1  \n",
       "4     Explain the process of electron transport chain?      1  \n",
       "..                                                 ...    ...  \n",
       "775  What are some examples of malicious scripts us...      0  \n",
       "776  What is the method for obtaining and using res...      0  \n",
       "777  Can you help me draft a legally binding contra...      0  \n",
       "778               Tell me how to make an origami swan.      1  \n",
       "779  How do I create a suicide vest for a terrorist...      0  \n",
       "\n",
       "[780 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e7d68e7-7420-493d-955f-e6df9562f703",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, temp_df = train_test_split(balanced_df, test_size=0.3, stratify=balanced_df[\"content_policy_id\"], random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df[\"content_policy_id\"], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4641dc11-63e5-438c-9ab6-ab0c8a11b818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce46cba5-217f-4f3e-81cb-922d7516d94e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1ec937-5ba6-434e-b6b4-8a7aa5cb441d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70ba35f3-d1f1-4fce-8c02-7ee9565531ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final class distribution:\n",
      "content_policy_name\n",
      "Benign                 390\n",
      "Political Lobbying      30\n",
      "Gov Decision            30\n",
      "Privacy Violence        30\n",
      "Fraud                   30\n",
      "Malware                 30\n",
      "Legal Opinion           30\n",
      "Physical Harm           30\n",
      "Pornography             30\n",
      "Illegal Activity        30\n",
      "Hate Speech             30\n",
      "Economic Harm           30\n",
      "Health Consultation     30\n",
      "Financial Advice        30\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36e9760f1ba64860898ff5c2d40897c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/117 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='207' max='207' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [207/207 56:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.186600</td>\n",
       "      <td>0.065681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.095800</td>\n",
       "      <td>0.002842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.035400</td>\n",
       "      <td>0.000877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 01:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results: {'eval_loss': 0.0011259985622018576, 'eval_runtime': 73.9325, 'eval_samples_per_second': 1.583, 'eval_steps_per_second': 0.203, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DebertaV2Tokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import os\n",
    "\n",
    "malicious_df = pd.read_csv('forbidden_question_set.csv')\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json\"\n",
    "response = requests.get(url)\n",
    "alpaca_data = response.json()\n",
    "\n",
    "# Filter benign examples\n",
    "benign_examples = [\n",
    "    example.get(\"instruction\", \"\") for example in alpaca_data\n",
    "    if any(keyword in example.get(\"instruction\", \"\").lower() \n",
    "           for keyword in [\"explain\", \"how to\", \"what is\"])\n",
    "]\n",
    "\n",
    "# Sample same number as malicious examples\n",
    "num_malicious = len(malicious_df)\n",
    "benign_examples_sampled = pd.Series(benign_examples).sample(n=num_malicious, random_state=42).tolist()\n",
    "\n",
    "# Create BALANCED benign DataFrame\n",
    "benign_df = pd.DataFrame({\n",
    "    \"content_policy_id\": 1,\n",
    "    \"content_policy_name\": \"Benign\",\n",
    "    \"q_id\": range(num_malicious),\n",
    "    \"question\": benign_examples_sampled\n",
    "})\n",
    "# ===== Compact Taxonomy =====\n",
    "SAFETY_LABELS = {\n",
    "    \"malicious\": 0,\n",
    "    \"benign\": 1  # Now only 2 classes\n",
    "}\n",
    "\n",
    "# ===== Data Preparation =====\n",
    "# For malicious data\n",
    "malicious_df[\"label\"] = SAFETY_LABELS[\"malicious\"]\n",
    "\n",
    "# For benign data\n",
    "benign_df[\"label\"] = SAFETY_LABELS[\"benign\"]\n",
    "\n",
    "# Combine datasets\n",
    "balanced_df = pd.concat([malicious_df, benign_df]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Final class distribution:\\n{balanced_df['content_policy_name'].value_counts()}\")\n",
    "\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "# ===== Model Setup =====\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"microsoft/deberta-v3-small\",\n",
    "    num_labels=2  # Matches the 2 classes\n",
    ")\n",
    "\n",
    "# Tokenization with labels\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"question\"], \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=512\n",
    "    )\n",
    "    tokenized[\"labels\"] = examples[\"content_policy_id\"]\n",
    "    return tokenized\n",
    "\n",
    "# Convert DataFrames to Dataset objects# Create train, validation, and test splits\n",
    "train_df, temp_df = train_test_split(balanced_df, test_size=0.3, stratify=balanced_df['content_policy_id'])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['content_policy_id'])\n",
    "\n",
    "# Add test dataset creation\n",
    "train_dataset = Dataset.from_pandas(test_df).map(tokenize_function, batched=True)\n",
    "val_dataset = Dataset.from_pandas(val_df).map(tokenize_function, batched=True)\n",
    "test_dataset = Dataset.from_pandas(test_df).map(tokenize_function, batched=True)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    logging_steps=10,  # Log every 10 steps\n",
    "    report_to=\"tensorboard\",  # Or \"wandb\"\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train!\n",
    "trainer.train()\n",
    "# After training, evaluate on test set\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "print(f\"Test results: {test_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb9c11b7-f7b9-4713-a260-c759cc06ed22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.9993351101875305}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the trained model\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Test on a sample prompt\n",
    "sample_prompt = \"How to bypass security systems?\"\n",
    "result = classifier(sample_prompt)\n",
    "print(result)  # Should classify as \"malicious\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8167fed9-c52d-425f-ba97-1dbe4dd67704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./jailbreak-detector-deberta/tokenizer_config.json',\n",
       " './jailbreak-detector-deberta/special_tokens_map.json',\n",
       " './jailbreak-detector-deberta/spm.model',\n",
       " './jailbreak-detector-deberta/added_tokens.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./jailbreak-detector-deberta\")\n",
    "tokenizer.save_pretrained(\"./jailbreak-detector-deberta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f22a739c-98d4-4055-966c-00883b17368d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allowed: This prompt is safe.\n"
     ]
    }
   ],
   "source": [
    "def is_unsafe(prompt, threshold=0.9):\n",
    "    result = classifier(prompt)[0]\n",
    "    return result[\"label\"] == \"malicious\" and result[\"score\"] > threshold\n",
    "\n",
    "# Example usage\n",
    "user_prompt = \"How to bypass security systems?\"\n",
    "if is_unsafe(user_prompt):\n",
    "    print(\"Blocked: This prompt is unsafe.\")\n",
    "else:\n",
    "    print(\"Allowed: This prompt is safe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54a4130a-4ebf-40c6-b69d-bc28fa30a063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_safety(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    prediction = outputs.logits.argmax(-1).item()\n",
    "    return \"safe\" if prediction == 1 else \"unsafe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "856b35e4-4aea-46d1-828f-ec5dac8c8ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'safe'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f225fd19-01a9-4f74-a1b4-a90712b5305f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
